{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGqLgzFW6tYb",
    "outputId": "fc15f28d-b411-4787-9684-e8011e403289"
   },
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2RKx3y96waD",
    "outputId": "df4dd7a6-105d-4164-ab11-550c64d15a94"
   },
   "outputs": [],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q19xvE46CqzP",
    "outputId": "18ca8ca3-52df-4d35-ef4b-e5aa01d34cc0"
   },
   "outputs": [],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDyikaRtA4bo",
    "outputId": "c28d9e02-2c4d-441e-c09d-828c06097187"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/\"MA4 semester project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFTjOCU6kZ7p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import pypdf\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "from io import BytesIO\n",
    "from utils import ChunkLoader, RAGEncoder, retrieve_relevant_chunks, get_all_chunks, show_chunks_statistics, get_index_and_chunks, get_formatted_prompt, generate_answer\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRpWRN1Xj3U6",
    "outputId": "5e5e3070-785a-4440-a11f-8af86b892235"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on: \", device)\n",
    "\n",
    "load_dotenv()  # Loads environment variables from a .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "d7d6a66115af46e987212eb809f40fbd",
      "9c9c3cdcc65f417fa7d3854dea2ffc50",
      "116df226bf8245b49bcf25c71bd3c06d",
      "802b39ab66af4b35a63b30b65c24760f",
      "ffefe79b82ce4579b834f24bbce281d3",
      "f8de622ddc414e389753421142facb8a",
      "9ab40b89a1c9433390e1c648f58da900",
      "5779a798f5e94b4babed9a4c643ea2f5",
      "597e56b5d4db40a4a4f849af07911ded",
      "2f85cfb9575d495b91e3cdb4e5a4e2b7",
      "9e4267c666174946a3c1fb99fc35bec4",
      "5f6929e04af24377a771a9ee5e53bb83",
      "7061d30b7eee42c19024f7501d3f66db",
      "e308a53264e54d5283fc60bc9141349f",
      "0ab2efdec00846f890316f96183edaf2",
      "28fb163947bc43f39d1a0a46df545b09",
      "822fee2c82254438b06c17ad4ac58538",
      "ba294e7d3eec4407932c3abb41df1b23",
      "ad3f81be13a2433596bee87c590a4563",
      "421022915cf84936afc09c3b53c93b90",
      "9a8c7c2a5ab94efb8be4432eefaab022",
      "279a22de4c7e4dd7a87ffa454038e0db",
      "e9aa70c5db584ab1b158c91e992408a1",
      "3c5c879bafaa414a90194529a104fb45",
      "f0198453b57e4283819e920e9fe24d47",
      "db948f5e0480412e8ebe543748fc5482",
      "c98d4ca50e5f49a09023b87baa9e2eeb",
      "71fb929299ee4b6f90939317260077e8",
      "cdcf6144093946e08c0ab098b02dd3ff",
      "56caa67ec0394ef48f317f9c6ff82819",
      "405dd3e623954ed0924e8ca15de34a9c",
      "5d88cb4e56394ed2a3852c636b2072cd",
      "dc37bdcd5d67465f85728d72ab491a14",
      "1ca01711f18841ad988382afcbd3204a",
      "fdb48af873964efea1cfbd0a3050d7f1",
      "1261771f3bda4b208d8fbb3d9ed3cb56",
      "6f62a0eb7a9b455094e343ab55dda4fd",
      "d3d1425da83747f6ba3e4cea44d687e1",
      "8875513df7d4457eaaef03acbf9160ba",
      "cf29e5c549ef4f7aa1bb06ed69c80ec4",
      "bbb34a3a5f4a432c85f2885d20b15859",
      "333abb4427ce4c189f464dab92c157a0",
      "f3d5e450559b496f8236d633349f04f7",
      "f9c9e84eedaf4753bdf81a19a0b02dbc",
      "abf06efc6d274b7594428ae20365e171",
      "c4cd23abd10e4c73a5983b8b55a67f65",
      "296ebfdd6fe0430ca1e1ea5523eba384",
      "6b876ccb1de248469e0e1710aef017d2",
      "edba69a4b5f84244948dd75f5ba268b5",
      "18a1292380bd4879a6087e666a91e34f",
      "1208bee7058c4fc7985a65df74bf3962",
      "6cc323a6121b4f0d9462a759c346e0fc",
      "04c30617160d4214b67b2bfc977c00c3",
      "517601d2ce5f4fd88ab1881332da77b3",
      "b43cb108cd3d4aa9882dc91b8eea064d",
      "58fe979eeecf4f9099463bdd5bfa9dd8",
      "5a703dfcc7f44de78680bbb45fa5cae5",
      "e226c1bcec954cdabe0ceae4e176d353",
      "bec88b033fb444cca3d4f59e2f8e2102",
      "c854da6008f5489cb64b530e2289eb64",
      "27e689d8248b4ba6b5beb1ac6fd5bd90",
      "b00633af7cd04396b64235e1116b438a",
      "40a8d227cc1d401fa4491a5e371e827b",
      "0be6aa9acb244473a01e9f20356fa7cd",
      "1c49fe5de0aa4a028cd4f4a30cf3376e",
      "14f5acf31beb4d31811f85a20bddb134",
      "7b0d4a7120224e93bbb2ca294f598519",
      "33f752c2146344239a020e7f2d0e3a5a",
      "0d47d552ce074f68b6b4d3ef85d49a0a",
      "540571d785ac44afa82f6290d743b7b8",
      "85f2e41953b0453da8533d2286c0aac9",
      "261255670928403cbc34a44a5a714f69",
      "a27cfc814712496a94818ee09d033462",
      "a493845ff1504dfc9cfff2c2c7d0dddc",
      "5402dca597e64d6ba355194a50940db9",
      "2dc2eaa8ee244dbaa96a377e39c53805",
      "603285bb84864df48c2dc43318b8c404",
      "0d2183106cb44f5d926859ffd475120f",
      "ac368a6acf994dca8548fa1dceadc05d",
      "0e68f49d07814107b8fa9e7fd87455e1",
      "cf8a33980e7f4002ae25278d6420d6f6",
      "8a8900e8e684432fb77ab542984ae27e",
      "8109820a7bff4e78a5c997f90a5e28cb",
      "14fbed1228b848a589bf57992030cc67",
      "d448f1fdd20d42bd9841ab423ccadcf7",
      "3e854e8facca4f00ac95fa513d2141c6",
      "0c13e9b6a08e49828a822b20cfed6331",
      "50c1db6f46bd4de19a21c96040785ecb",
      "660274bc8f1f4fe4aba2a94f0d9fe8a2",
      "07a97ee9a11147e79534350aaf7772b8",
      "f63d1284dd0d4d6193ad0c0fe9ff118c",
      "3116baa381704f48a368170d2e04e3e4",
      "046c6dbcc9f44a9fa9a62dcf3e01d22a",
      "e7238b448de6470db30a2b1d7e6dbb20",
      "dfb0586db68b413285831701189a51f2",
      "8e0eb8650c64440f89f01ccce86f7676",
      "1415d4251954418d9f020dd4dd6b722a",
      "ae6b950c99bb444b8734b412f6718664",
      "fd9a0e404e2b4a43b1cc3c4a530c09c9",
      "90e81bd31a6c4c958f49290af086633b",
      "54286d2dbbe348f2b534bcf6738173dd",
      "68b137b8789f4930be84abb1a386b234",
      "619d8e133bf64b69aeed56583da66645",
      "1f936f669e9c44d0884ecd6fea23d8b2",
      "5fd871a710ee40899304895aa59eda3d",
      "e65dbf5ae8f04ba8843ffcd59f4d537f",
      "5c143e66dcf24b3f9912bf10565ae76e",
      "3d4a034a4bad4f7592c3a1593e9e6965",
      "a8527f9883b546988c2a3c8cfb287868",
      "09c057598d7649d8816f73cf2f197dda",
      "adad33d62f9141748432ca7a3f48cec2",
      "88d4a6bd6b914eb88f27579929d71e4b",
      "60d62552e2a24565a1c8306c28261209",
      "d53f16b43ef04fda89a1e0e606559776",
      "deeed144d5b244b98e33aa8326dcf0e5",
      "14ec6a90023e4a6cb343b38a0e59c679",
      "0d79d9ee81184e8a939d788036a2155c",
      "a26f9b4ad1ce41dcbe1bac834079e342",
      "63180a5f8f4b4e18bde708476f060ccb",
      "173beff3cf994ed09b047c1d76c28269",
      "e21d8ab6292447feaffe707e63a8cad5"
     ]
    },
    "id": "YA-Mx0-oW2Ta",
    "outputId": "0a15bac5-b3a1-4dc9-961d-8f15e93d4093"
   },
   "outputs": [],
   "source": [
    "MAX_CHUNK_SIZE = 256\n",
    "OVERLAP = int(0.2*MAX_CHUNK_SIZE)\n",
    "\n",
    "# # Setting the LLM used for chunk embedding\n",
    "# chunk_encoder_tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "# chunk_encoder_model = AutoModel.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "# chunk_encoder = RAGEncoder(chunk_encoder_tokenizer, chunk_encoder_model, device)\n",
    "\n",
    "# # Setting the LLM used for question embedding\n",
    "# question_encoder_tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "# question_encoder_model = AutoModel.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "# question_encoder = RAGEncoder(question_encoder_tokenizer, question_encoder_model, device)\n",
    "\n",
    "rag_encoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# rag_encoder_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# rag_encoder_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "rag_encoder = RAGEncoder(rag_encoder_model.tokenizer, rag_encoder_model, device)\n",
    "chunk_encoder = rag_encoder\n",
    "question_encoder = rag_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xntqVHNRXSGK"
   },
   "source": [
    "# RAG preparation: chunking, embedding and uploading to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e27c787fd76542ea8bb79957a896ebae",
      "cce7a364fc9241459ca99a2eb273772e",
      "20f4efc10e80403080537a8b7223a234",
      "095e213217d44904bd2d80749ede62c4",
      "74e35fa4032d478b84c66f2033ced3b0",
      "e1975fb177714362b4ea3291b59f941b",
      "467868fa151143ffad845406f3204af3",
      "7b6d3bbeb5674b59bab557fdbbb11cc3",
      "cf2bcd46f8ac418489a1355fe6b51df7",
      "28be7e5a20b2461e97cde9c413b8dd9c",
      "80be6f7259fb46f0acb9e6712f1443d4",
      "c5bd1ffae85a4b55ae436eef8aa6671d",
      "fbe1a166935546a795d4776a4ad7b476",
      "8435f136da9f46d987cd40b566627692",
      "747d4a58cc75430383624df6ac321b9b",
      "efee874c026b450290ce45521b9f2c40",
      "840094ad48ae419299cd68c7af453a53",
      "dffc863de792469f87602f82e8a96d93",
      "510096c626ed42adaf069f44e01bc0f7",
      "d362e4eeaaa04b7f9da99acbf51f484c",
      "9273a790f1b54fe3aff66689ee255e47",
      "9c40bd22496545a99f51f548481a9a17",
      "4334403e376e4a9c83b9b9fdf0588675",
      "952cc89101d94eeab4028790609ad3bf",
      "88732640fc1b49f5bcaccd6863b197cc",
      "d4ffc9f82dc546529bb1c44f0803fa07",
      "2f7ca901287f4f9abe32b053a87d03da",
      "cea780e1d9da4553a1dc8827e4d6627b",
      "bcc23138b25f4467aabbb1f175eb5b1b",
      "7d3c4571db29468d97e8ddfe9e62c1d9",
      "d014eaa5345b4338945466510d36c91a",
      "42d66ff52500408684520b4b917e3f8e",
      "a7e02a8113264f72ad4052838a9bb07c",
      "983922b5b0d149e999bf9a67916d56dd",
      "55f5ffd1ba5141beaa39d93947edb8c5",
      "3824e83ff37747bbb1aa1d7b4efed27e",
      "fae908737cbb4e78bc44c6216a779774",
      "b4eb05f5a53f4bc7a72f2e78bac2d275",
      "e01c9b89310d4f928b6264c6d63a3b1a",
      "4ca5a5fb420b42d080e262ddc80fd511",
      "6436136c2f2a41179bf96bcfe5805768",
      "96f8543cb5654c949f893920b71c390a",
      "dd67c1c9be8449aeb01c1d45f7d9381a",
      "cf43ae37df604507b4e29af415a976f4",
      "348b064ad93742f5a0bb39aeef277771",
      "1077866eabbe44f6ac6ea46c9f99edb3",
      "b74da176c92544c7aab52a3d85abd1dc",
      "6096e620cf02428db228dd7964febce8",
      "99a1e72004bb4623a80a7a257749ae0f",
      "c2526d145db14d35b7d66de463366ee4",
      "e42f7387d4224167a8b5d0db4f36e41d",
      "374962212ab1438aab414e6f89fffaa3",
      "1e8c5bd875e440f4a5114ee6ba1d2f9b",
      "98eed741a4a54783a5641c91009de115",
      "4220001bc8ca42058f9db54a298d4fbd",
      "aae5ac54ff6741cfa6f2ff1cc91e315a",
      "7ca87299842c4ebfbea1bbf251daaec0",
      "b640385f778f4a088a07887a185222bc",
      "0dd5c03ec5d1429b96efe9629a4f2e47",
      "2f2deaf237f547e681ab863bbfcd96c4",
      "7f1d5be0ea6c41eda68f8f7f4c1119a6",
      "ced20c3a518246fcb071399ab15809ed",
      "a646570ff78c4523a94224675ee1a84f",
      "519d522d7c584d3a84c78a3bb4e3a9cf",
      "4b13b72f5f8548fc989426869a4ac93c",
      "574f960ab68b4f93bf4476f917907302",
      "cc630235786a49378151bf9b1b3c4c47",
      "017d833f45144f4ca089f4d53b8bd02e",
      "d1182f6fc9d64212a4e6b03e05c63430",
      "baa1105e6a6b476bbc1d3e9325c44b97",
      "bb08414cf3dd4f6d8217c945093a5982",
      "cc37ff4a18994ad28a9c7cde5aa0939a",
      "a9a644c7aa1d4543a964dbb5df2467a1",
      "4d551ee86c2243e093e5034d6f9e1fff",
      "e4c425927a424c308f50e50eab29c70b",
      "9e54a8eba22240b28258ed546c8c05b5",
      "5562d7c8905f4318a81c2768626631e5",
      "b3154490bb384715b18a844e8384f2dc",
      "16b4a33f864b49d79d82c0e560381a75",
      "619c6addd1cc4aad895030cb23bcdbfa",
      "543b20d5718a45c7956e6441efc72c46",
      "9cfc011e2c604cc9bd85f31a9146ef08",
      "c04af46df2dd449bb7bca0dc47ac23bc",
      "c054c3bd65a14edc8d2a4002d3708a6b",
      "af29e657af934a79b294fe9ff0161f88",
      "c787e174f24e433b9c7fe828967353ac",
      "fd46a6aaf373449cb79fb029519f4822",
      "991a7b71b1134e0b96325e05e2d3c940",
      "b85891f6e35b4ffdb56a77fc5b1fdcf8",
      "8a6c0c62c7a042bebbe1a9429ff38ff6",
      "bff712622a8c4598a179e70a3e73168b",
      "df772fa5a8244f7597da9e210fcfef51",
      "8639afbedc124dd5aabcf0a2f21ff109",
      "76cc559afea14629a6a43c8411b79bd7",
      "677bf3aa9c234994bdb17668a5f2cacd",
      "cc391f1c6ba94cca9ea1b7339c905242",
      "a939397f0eff45ae96570e0b413a9488",
      "5d2d820fa1a8434e8baa60c83e5a6af4",
      "c8381a8c740847359225c9bb5dcda7fc",
      "15d4f81970c04c12b61878d98e5d76d8",
      "ba9b6061451440b4ad6385e42aaa01cf",
      "14066456bdc64a7ab024a0de62e9d052",
      "bae42b42659e4def98cea297b2ddcc82",
      "0f1b2da8f5834751882e75c92b3bafa5",
      "15c968b2e61849f98a15aa028fe20a0a",
      "aa40ae02a4a64a91a9084d349aa074f6",
      "002b0dff73214b008b76e13bbe996c79",
      "50c123e2a947422fbf083872518d0055",
      "8cf40c20403e4f50a4aef4272a261d4e",
      "79ba1e7fea3e4ed0bbfcd360f04c75fb",
      "4c25faa7eacb4e61b1f0b4cfab11aa64",
      "662d0b92160c48f68e13c6874deff860",
      "ecc3cfcaaea54c74b6e5918870c2eda1",
      "22354c62dbd04bb3991e8f8cf1548c56",
      "e547470025f7410ba6ac9056972191cc",
      "ceb06427a9454d8180c1f802b7c7b71c",
      "9c0458ab0a1c410e945eb37c19ae90ce",
      "b2a456a795294b7cb78e00779029990c",
      "e68e28c036414daa80fd5202fc228279",
      "c318ab0ca4e941f69f8b2d81fd94c84d",
      "8b9fefb7f0d94b2cbdfa136849934ffc",
      "0858c67366c04f3a804760c41e60f2e0",
      "b747e093524c4a178ad02946373f63ee",
      "c4f74bf8212448f99cc57ab9a6e7dc19",
      "7297c7d7e8b74d43aa2d703f71512683",
      "728748ad053a492ba8c33fa5a8e5cd5b",
      "acbb450dad65475bab90e9eba4002b14",
      "abe865d8436e4ac981c0d8603c4370e4",
      "068f03966e5744c18c65ca31a0dffc5e",
      "d53b3a6f182949a3b083316e79bab0d3",
      "27e427a47b824476b7dbb2404a8357a0",
      "e60ccc1bead640568bb57aae240dc03c",
      "84b3d0c2c9e94a86a603247583aed31b",
      "48b4a00770f343adbb98eeb9459fc044",
      "d4cfbad050984aadadcea46ad03db2d5",
      "405cfe68eaff40e1b82c32d54739d94d",
      "259886134c7340b4838d096f20ee5155",
      "4429213a669243a2be677dc0711109de",
      "c856371adcb144b4bcd9263e642afe9e",
      "86d49ff895544e0284cc780b20d8f9c2",
      "fcf14c3bf98244048aa548579f2bc9f6",
      "088e06768ef44903b3f72fdf01159cad",
      "f36e5e8e00f34ba4998682303855a326"
     ]
    },
    "id": "U04de2nmWuHP",
    "outputId": "91eae35e-06bb-4ab7-8b22-8090391905a2"
   },
   "outputs": [],
   "source": [
    "# folder path containing subfolders which contain the rag documents (the parent folder is divided into subfolders with different timespans)\n",
    "PARENT_FOLDER = \"./RAG documents\"\n",
    "\n",
    "def rag_chunk_embed_and_upload(parent_folder, rag_encoder, max_chunk_size, overlap, prefix, hugging_face_write_token, show_statistics):\n",
    "  api = HfApi(token=hugging_face_write_token)\n",
    "\n",
    "  for sub_folder in tqdm(os.listdir(parent_folder), desc=\"reading parent folder \" + parent_folder + \" ...\"):\n",
    "    #chunking the documents of the current sub_folder\n",
    "    all_chunks = get_all_chunks(parent_folder + \"/\" + sub_folder, rag_encoder.tokenizer, max_chunk_size, overlap)\n",
    "\n",
    "    if show_statistics:\n",
    "      show_chunks_statistics(all_chunks, rag_encoder.tokenizer)\n",
    "\n",
    "    # transform the chunks list into a jsonl file (we use BytesIO to avoid having to store the file locally)\n",
    "    all_chunks_jsonl_content = \"\\n\".join([json.dumps({\"chunk\": chunk}) for chunk in all_chunks])\n",
    "    all_chunks_jsonl_file = BytesIO(all_chunks_jsonl_content.encode(\"utf-8\"))\n",
    "\n",
    "    # upload the jsonl file\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=all_chunks_jsonl_file,\n",
    "        path_in_repo=prefix + \"_chunks_\" + sub_folder + \".jsonl\",\n",
    "        repo_id=\"ziedM/rag_dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    # vectorize the chunks (again we use BytesIO for the same reason as above)\n",
    "    all_vectors_npy_file = BytesIO()\n",
    "    all_vectors_npy_content = np.vstack([rag_encoder.encode_text(chunk) for chunk in tqdm(all_chunks, desc=\"vectorizing the chunks ...\")])\n",
    "    np.save(all_vectors_npy_file, all_vectors_npy_content)\n",
    "    all_vectors_npy_file.seek(0)  # set the offset back to the beginning of the stream\n",
    "\n",
    "    # upload the embeddings/vectors (we choose numpy as we're using faiss vector DB for similarity search later in the RAG pipeline)\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=all_vectors_npy_file,\n",
    "        path_in_repo=prefix + \"_embeddings_\" + sub_folder + \".npy\",\n",
    "        repo_id=\"ziedM/rag_dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "rag_chunk_embed_and_upload(PARENT_FOLDER, chunk_encoder, MAX_CHUNK_SIZE, OVERLAP, \"sentence_transformer\", HUGGING_FACE_TOKEN, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdrw1Dk3Xhe2"
   },
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jt1tz2auH06",
    "outputId": "22cc6426-828e-407b-f2a3-6a08cebf616d"
   },
   "outputs": [],
   "source": [
    "# Setting the LLMs used for generation (we are using 3 LLMs GPT2, GPT-3.5 Turbo and GPT-4o)\n",
    "\n",
    "# GPT2 from HF: (knowledge cutoff: November 2019)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(device)\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#The embedding size for GPT2 small is 768\n",
    "print(\"The vocabulary length for the GPT2 small model is:\", len(tokenizer))\n",
    "print(\"The number of parameters for the GPT2 small model is:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"The max sentence length for this model is {}\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The pad token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))\n",
    "\n",
    "# This OpenAI client is used to chat with GPT-3.5 Turbo (knowledge cutoff: September 2021) and GPT-4o\n",
    "# (knowledge cutoff: October 2023) through OpenAI's platform (as these models are not available publicly)\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ApEYYC3OOv_"
   },
   "outputs": [],
   "source": [
    "# instruction 1.1 is used to get the prediction and instruction 1.2 is used to get the assesment\n",
    "TOP_K_FIRST_MODEL = 3\n",
    "TOP_K_SECOND_MODEL = 5\n",
    "TOP_K_THIRD_MODEL = 7\n",
    "RESULT_DIR = \"result_sentence_transformer\" # or result_facebook_dpr\n",
    "\n",
    "role = \"You are an expert in deep technologies.\"\n",
    "instruction_1_0 = \"Answer the question below. Keep your response concise (maximum 15 lines).\"\n",
    "instruction_1_1 = \"Using the provided context, answer the question below. Keep your response concise (maximum 15 lines).\"\n",
    "instruction_1_2 = \"Based on your current knowledge, evaluate the accuracy of the following prediction about deep technologies in {}. Assess whether the technologies mentioned: - Had the predicted economic or societal impact - Gained significant adoption or traction - Were delayed, overestimated, or abandoned. Provide a short analysis of how accurate the prediction was, citing relevant developments if possible. Prediction: {}.\\nAssessment: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cZ3M9_GSMDV",
    "outputId": "6d568c03-7ca3-40e6-aabe-61fa04395f6a"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"Answer the following in three parts:\\n\\nTop 5 deep technologies by economic impact (for the year {period}).\\nRank from highest to lowest impact.\\nFor each: name the tech, estimate the percentage of economic impact (e.g., cost savings, productivity gains) the technology will have, with 100% indicating a strong effect and 0% indicating no effect, explain briefly.\\n\\nTop 5 deep technologies by societal impact (same period).\\nRank from highest to lowest.\\nFor each: name the tech, describe key social changes (e.g., health, privacy, ethics), explain briefly.\\n\\nTop 5 emerging deep technologies.\\nRank from most to least likely to become impactful in the year {period}.\\nFor each: name the tech and briefly state why itâ€™s emerging.\\n\\nOverlap between lists is allowed.\"\"\"\n",
    "print(question.format(period=2021))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhO21ZjBXXdB"
   },
   "source": [
    "# Idea 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6Ha2PNNXbhW"
   },
   "outputs": [],
   "source": [
    "# prompt 1 is for gpt-2 to get prediction 1.\n",
    "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
    "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
    "# prompt 4 is for gpt-4o to get assesment 2.\n",
    "# prompt 5 is for gpt-4o to get prediction 3\n",
    "\n",
    "prompt_1 = get_formatted_prompt(role + \" \" + instruction_1_0, context=None, question=question.format(period=2021), RAG=False)\n",
    "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
    "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "\n",
    "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
    "assessment_1 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_2}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "prompt_3 = get_formatted_prompt(role + \" \" + instruction_1_0, context=None, question=question.format(period=2023), RAG=False)\n",
    "prediction_2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_3}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
    "assessment_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_4}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "prompt_5 = get_formatted_prompt(role + \" \" + instruction_1_0, context=None, question=question.format(period=2025), RAG=False)\n",
    "prediction_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_5}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "result = {\n",
    "    \"prediction_1\": prediction_1,\n",
    "    \"assessment_1\": assessment_1,\n",
    "    \"prediction_2\": prediction_2,\n",
    "    \"assessment_2\": assessment_2,\n",
    "    \"prediction_3\": prediction_3\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR+\"/\"+\"results_idea0.json\", \"w\") as f:\n",
    "  json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RYzC6EUc6ok"
   },
   "source": [
    "# Idea 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "f9a339b51d65442c9c9960698b5accd4",
      "4c95b12da847476a85ce663b44733214",
      "7ca5bce5afb941ad99340a92a393aee7",
      "9305fb93b01a49d68b99a8ba80c9b548",
      "ddf0d4cd82b24caf9f8199b9f4ebee37",
      "1a64493713aa4d95b758420d20fe2349",
      "5f823d273230435698e0d3bcc9cff288",
      "829f82ba231a48049014c94eb54d673f",
      "301783059ac44620837e17b56ce5d085",
      "8112947443c94e51bf4411681b2eaf35",
      "2fcce8deca574a7ba4f2e7b6e4f36704",
      "7b75f33a0d32460c92e273e59059e861",
      "78be3a0525954732b2dd0f1afd5b8d63",
      "e3e03bfc2fe84fa588038fb2ed75a938",
      "06ac221198c8470fb9ff431de0d0b553",
      "b1f4f54747aa435cad168c7472ae7616",
      "350b9f76a4ad45b5b4b37a87d476a215",
      "0cc6aee3b2b040e9b99e74b49ac2dee1",
      "fa1c1ba06c54488b9460a5f1c941a1ae",
      "2c3381d292694ea0b59580fbd5f96b46",
      "1bfafbfaeadd4706b7a383f463cb262d",
      "c2fe4057f5284477b5efe0160022687e",
      "21dbda9240ac41899286ac7c71a5b7e3",
      "163f52b86d0b4eaabfe6742071758d95",
      "6c97cb93a5ed40caa5bda855d54ff921",
      "ba8b5dc4b3f648a18077e3f8702d425b",
      "91d9d4f108954ef08ac46c6231b25efc",
      "f4f5cfbb80324eba9503dc66df373f43",
      "e282d55eb2dc42e3aba089f4a5a784ed",
      "7006158cf90d4fc9bed8d1e71c6d3877",
      "d2f3e5e39ceb47a79fce5fa83d3073d9",
      "0854df75a77f4dcb84859072e9514abb",
      "0868b4021fa14115b13feab2ef19820d",
      "bd890c5297b74fc881e0f1325962a44d",
      "11df82b840634120a9da341fbcb681b5",
      "6c5ac7ca664e466185576d646b89651a",
      "7b45efd8de3244c4ba8ea342ac0d55af",
      "026d652803404a14aa4f65bbf4c4009f",
      "875f74a21e694aefb68a3a514857b120",
      "be1889d0c89540bb84a1dbc06493e9bd",
      "fe1317c12cd74011b0301195604567fe",
      "16e7ce5c5a8243bc99869c5f9ffed702",
      "49bfbfd138f146e1a68897299c53b110",
      "7dd2f26c9c8842a1a41f856d2c407609",
      "cf2cf9a901c14becb76b067a4a57aca7",
      "24d744ae73484fb6a97f630d8d5f3b01",
      "f5807cd4e49242528a770e30cc37e243",
      "5ac2687f48cf4ad5b7c7cdc23191dde8",
      "8a779b18de9b433685c558a90fd5b682",
      "f1a0f105318d4386acc40d524d5b76eb",
      "fbb77f8c05184e99a9cff8a6461daafb",
      "8252fa84bdaf4f719777cd7e614790a0",
      "4698e05f22db40b282171c4c1dfcb028",
      "d6932e18b5d849b283f65c9fbc00cbf9",
      "3b8954979a854f4687ad18b8e9b0ea97",
      "057460278c004a6b9a4589e145bfa6b5",
      "487cf1fed7b94e9a9faca776be926c7a",
      "56864a8a585f46a388607fb27fbabcda",
      "fc7a78a26439474bab84d9ac11fb19d2",
      "8efa09e077e442a8b5bc42fd490bf998",
      "849b20bae8234bc2a9fb9b40b1df4e6e",
      "60917b44e2bf47458c682ed506c306f4",
      "341abd6decac45299d4a55e315ce99d9",
      "1f5179d8bc7e4226b04776f4b3167e2d",
      "a87b96b7b8df40ac9da69259f1dc1516",
      "cd059ec4f68c470d868a26e568e2ef4a"
     ]
    },
    "id": "t8yP08Ixc-QU",
    "outputId": "4fb4d038-6896-4f81-c48d-f78331bb7fff"
   },
   "outputs": [],
   "source": [
    "# prompt 1 is for gpt-2 to get prediction 1.\n",
    "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
    "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
    "# prompt 4 is for gpt-4o to get assesment 2.\n",
    "# prompt 5 is for gpt-4o to get prediction 3\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2019_2020.jsonl\", \"sentence_transformer_embeddings_2019_2020.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_1 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2021), RAG=True)\n",
    "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
    "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "\n",
    "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
    "assessment_1 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_2}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2021_2022.jsonl\", \"sentence_transformer_embeddings_2021_2022.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_SECOND_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_3 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2023), RAG=True)\n",
    "prediction_2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_3}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
    "assessment_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_4}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_2023_2024.jsonl\", \"sentence_transformer_embeddings_2023_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_THIRD_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_5 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_5}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "result = {\n",
    "    \"prediction_1\": prediction_1,\n",
    "    \"assessment_1\": assessment_1,\n",
    "    \"prediction_2\": prediction_2,\n",
    "    \"assessment_2\": assessment_2,\n",
    "    \"prediction_3\": prediction_3\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR+\"/\"+\"results_idea1.json\", \"w\") as f:\n",
    "  json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMd6KDvFODQo"
   },
   "source": [
    "# Idea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "5bbd5543cac5479d8c5adca25c28776e",
      "ab518ce3fd384dbf970f58da40a79729",
      "f4c9e71fd85c4e238956c6a43dba5ab4",
      "1169312b875a4539a445f15255f38b58",
      "36a019a9d02a407b887b82ca78b1891b",
      "ad6dce95b5a342aabaeb5c75f16a1158",
      "23818f63f68f478fbb200a0963c51b47",
      "c5b0ad6dc5074760885fd5cb47a45f00",
      "ba795bbd09fa47e0ba464ad45a38974a",
      "753a638cb94045e28db9cfbce9fe6680",
      "3a798bce4db947f48cdfc7bd9784cbbb",
      "67e04219913b40c4af981e4c80ca15e5",
      "c58c5a5138d747b99490fcf0f3928e07",
      "ec3702502afa4eee8fbbca8f8be437cf",
      "fa471dcd35ba4b59bf01ae7dc847b411",
      "627b520adc454c01af88ebe7e3b875c1",
      "4691f0407a1a4c678e3e44eeddeb6f18",
      "881ed554021649649a2abf4cb76b330e",
      "55bc7b3ad11c42f7b39b6c703fec6a29",
      "5b3193a31ab24ca3983b000a667e318d",
      "66a6be09b2dd4403a089c7f961c0ae19",
      "b1fb9e06dc4540ed87862cf25031f198",
      "719396a592e9479c90f8b4adde492e65",
      "fa2af80564354305a1cc375592ffe40b",
      "cceee067d016410db936d0b4bc2327c8",
      "6b91b31bcdbd4e69a230aaa27c158485",
      "459f80dde30e4871b2586836967245c1",
      "f12511fa8a6b43b4ae15d6a2d2f1e860",
      "8da4a6c9941b45c2be17e20f072340fa",
      "0d50944eb4d64c53ab65ca6aba5f2642",
      "25b09f8284b74bdabd6b00d26a1e32c6",
      "9dadaf926d634686b2058194807b2359",
      "0335a3ee3b72431c95a5198fb4f87955",
      "5fd5c0ae0a27415d80b7d43440c7f2ef",
      "742faf171a7b48dfad49698ee90d385c",
      "c6368f2f1279450e9c061dd7ac266527",
      "8ccf4f0d81214d819996894839579497",
      "6120e65b44b74b9eae250c89b24702e8",
      "4a5019aff2e84033be6c01d5e6238dc7",
      "69b2438fedc448d8af07c88549264673",
      "d5d5370b8ebb4c9da5fbb1677e0c3932",
      "d48e7ece668b4390a30e6eed76106d75",
      "5aa4b04f75a14847ad0d7edb86c1500c",
      "1a3c64c035794c9bb29ba0e5db0e9b13",
      "93892922ec9043389bc3e2faa555b3a7",
      "7351f99894c34f708b44f10f79692cbc",
      "965abd5427a045b1a0afcbc76937c7b7",
      "48de9046f1824b7b91bc17cf67cfe4b7",
      "7d0b42b7ce704998b9817298fc448110",
      "428b4e46bc8d44daa37573b17d2ccb2f",
      "ab399d03344147389ea1689faf36b492",
      "a916f0e1f1e649628367fa2bac6fec5f",
      "b363b7e955ea440b90340682704c61e2",
      "b33771ae6af24a529f4cef46a9548ee3",
      "1fb73613b9a246078ad4a028a6167c68",
      "3f83d02b863142bbad5cf4e7b03d6d68",
      "c7828f3c87e44cc5a96403736a59a16f",
      "0c744d26695047ac9e41dcbfdd9f1297",
      "5281a3d2937c424da45204221cf38cb9",
      "7c84bb9bffc843a9be58594535d32a4b",
      "1ef1253a925c486ea4ff129309af57d6",
      "ebf6fb1d5f1742578366a13efdb7d6d4",
      "ccb4e532917d48a8b30edf79d640f8f3",
      "19b57b64f94846e5b40b567c2df62e27",
      "8fcec944266644809c2b84a16ac8d7c4",
      "2d26c3e886344d489875fa1790ad0614"
     ]
    },
    "id": "3CrsQYAzOHLr",
    "outputId": "16a231e6-4710-4375-813d-bef858a01927"
   },
   "outputs": [],
   "source": [
    "# prompt 1 is for gpt-2 to get prediction 1.\n",
    "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
    "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
    "# prompt 4 is for gpt-4o to get assesment 2.\n",
    "# prompt 5 is for gpt-4o to get prediction 3\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2020.jsonl\", \"sentence_transformer_embeddings_past_2020.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_1 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2021), RAG=True)\n",
    "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
    "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "\n",
    "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
    "assessment_1 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_2}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2022.jsonl\", \"sentence_transformer_embeddings_past_2022.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_SECOND_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_3 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2023), RAG=True)\n",
    "prediction_2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_3}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
    "assessment_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_4}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_THIRD_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_5 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_5}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "result = {\n",
    "    \"prediction_1\": prediction_1,\n",
    "    \"assessment_1\": assessment_1,\n",
    "    \"prediction_2\": prediction_2,\n",
    "    \"assessment_2\": assessment_2,\n",
    "    \"prediction_3\": prediction_3\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR+\"/\"+\"results_idea2.json\", \"w\") as f:\n",
    "  json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEisNTepdnyB"
   },
   "source": [
    "# Idea 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iM0_rDyGdqWB",
    "outputId": "f4ba86f7-75a7-4248-906e-92c433f5a545"
   },
   "outputs": [],
   "source": [
    "# prompt 1 is for gpt-2 to get prediction 1.\n",
    "# prompt 2 is for gpt-3.5 turbo to get assesment 1.\n",
    "# prompt 3 is for gpt-3.5 turbo to get prediction 2\n",
    "# prompt 4 is for gpt-4o to get assesment 2.\n",
    "# prompt 5 is for gpt-4o to get prediction 3\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_1 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
    "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "\n",
    "prompt_2 = instruction_1_2.format(2025, prediction_1)\n",
    "assessment_1 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_2}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_SECOND_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_3 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_3}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "prompt_4 = instruction_1_2.format(2025, prediction_2)\n",
    "assessment_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"system\", \"content\": role},\n",
    "  {\"role\": \"user\", \"content\": prompt_4}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_THIRD_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_5 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "  {\"role\": \"user\", \"content\": prompt_5}\n",
    "]\n",
    ").choices[0].message.content\n",
    "\n",
    "\n",
    "result = {\n",
    "    \"prediction_1\": prediction_1,\n",
    "    \"assessment_1\": assessment_1,\n",
    "    \"prediction_2\": prediction_2,\n",
    "    \"assessment_2\": assessment_2,\n",
    "    \"prediction_3\": prediction_3\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR+\"/\"+\"results_idea3.json\", \"w\") as f:\n",
    "  json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Cuk6wseqmF"
   },
   "source": [
    "# Idea 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y93hrmlUetYd",
    "outputId": "4190f4a5-3a3e-40a0-c93a-0e55d795384a"
   },
   "outputs": [],
   "source": [
    "# only gpt-2 is used for all predictions and assessments.\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2020.jsonl\", \"sentence_transformer_embeddings_past_2020.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_1 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2021), RAG=True)\n",
    "prediction_1 = generate_answer(prompt_1, tokenizer, model, device)\n",
    "prediction_1 = prediction_1[prediction_1.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "prompt_2 = instruction_1_2.format(2021, prediction_1)\n",
    "assessment_1 = generate_answer(role + \"\\n\" + prompt_2, tokenizer, model, device)\n",
    "assessment_1 = assessment_1[assessment_1.index(\"Assessment: \")+12:].strip(' ')\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2022.jsonl\", \"sentence_transformer_embeddings_past_2022.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_3 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2023), RAG=True)\n",
    "prediction_2 = generate_answer(prompt_3, tokenizer, model, device)\n",
    "prediction_2 = prediction_2[prediction_2.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "prompt_4 = instruction_1_2.format(2023, prediction_2)\n",
    "assessment_2 = generate_answer(role + \"\\n\" + prompt_4, tokenizer, model, device)\n",
    "assessment_2 = assessment_2[assessment_2.index(\"Assessment: \")+12:].strip(' ')\n",
    "\n",
    "index, chunks = get_index_and_chunks(\"sentence_transformer_chunks_past_2024.jsonl\", \"sentence_transformer_embeddings_past_2024.npy\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks, question_encoder, topk=TOP_K_FIRST_MODEL, normalize=True)\n",
    "context = \"\\n\".join(relevant_chunks)\n",
    "prompt_5 = get_formatted_prompt(role + \" \" + instruction_1_1, context, question.format(period=2025), RAG=True)\n",
    "prediction_3 = generate_answer(prompt_5, tokenizer, model, device)\n",
    "prediction_3 = prediction_3[prediction_3.index(\"Answer: \")+8:].strip(' ')\n",
    "\n",
    "result = {\n",
    "    \"prediction_1\": prediction_1,\n",
    "    \"assessment_1\": assessment_1,\n",
    "    \"prediction_2\": prediction_2,\n",
    "    \"assessment_2\": assessment_2,\n",
    "    \"prediction_3\": prediction_3\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR+\"/\"+\"results_idea4.json\", \"w\") as f:\n",
    "  json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xntqVHNRXSGK"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
